\documentclass{report}
%\documentclass[10pt,letterpaper]{scrartcl}

\usepackage[]{classicthesis}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{framed}
\usepackage{hyperref}
\usepackage{fancyvrb}
\usepackage{color}
\usepackage{bm}
\usepackage{listings}
\usepackage{soul,xcolor}
\usepackage[htt]{hyphenat} % allows breaks inside texttt
\usepackage[acronym, toc]{glossaries}
\setstcolor{red}


% this makes sure that each section gets a fresh page.
% \usepackage{titlesec}
% \newcommand{\sectionbreak}{\clearpage}


\usepackage{xcolor}

% this makes links prettier
% TODO: find a nicer way to indicate that text is a link.
\hypersetup{
    colorlinks,
    linkcolor={blue!30!black},
    citecolor={blue!30!black},
    urlcolor={blue!30!black}
}

\lstset{language=C}
\lstset{basicstyle=\ttfamily\footnotesize}

\newglossaryentry{elf}{
	name=ELF,
	description={Executable and Linkable Format; a file format used to store
	executable programs. An ELF file describes the address at which each
	section of program code or data should be loaded.}
}

\newglossaryentry{cr0}{
	name=\texttt{cr0},
	description={Control Register 0. It holds bits which determine how the
	processor operates. It e.g. determines whether paging and protected mode
	are enabled.}
}

\newglossaryentry{cr3}{
	name=\texttt{cr3},
	description={Control Register 3. It points to the current page table.}
}

\newglossaryentry{esp}{
	name=\texttt{esp},
	description={Extended Stack Pointer register.}
}

\newglossaryentry{eip}{
	name=\texttt{eip},
	description={Extended Instruction Pointer; a register which holds the
	address of the next instruction to be executed.}
}

\newglossaryentry{eflags}{
	name=\texttt{eflags},
	description={A register which holds various flags that mostly reflect the
	properties of the most recently executed instruction. For example the
	signed flag is set during a subtraction whose result is below zero.}
}

\newglossaryentry{cs}{
	name=\texttt{cs},
	description={Code Segment selector register; it holds an index into the
	\gls{gdt}. Its lower two bits determine the \gls{cpl}.}
}

\newglossaryentry{cpl}{
	name=CPL,
	description={Current Privilege Level; the current ring in which the
	processor is executing. CPL=3 means ring, 3, i.e., user-mode, while CPL=0
	means ring 0, i.e. kernel-mode.}
}

\newglossaryentry{gdt}{
	name=GDT,
	description={Global Descriptor Table; a table which holds descriptors,
	each of which describes a segment of memory and its permissions.}
}

\newglossaryentry{pte}{
	name=PTE,
	description={Page Table Entry; a 32-bit integer which stores a physical
	address to which a virtual address maps, as well as some status bits.}
}
\newglossaryentry{mmu}{
	name=MMU,
	description={Memory Management Unit.}
}
\newglossaryentry{tlb}{
	name=TLB,
	description={Translation Lookaside Buffer.}
}

\newglossaryentry{pde}{
	name=PDE,
	description={Page Directory Entry; a 32-bit integer which stores a
	physical address of a second-level node in the page table, as well as some
	status bits.}
}

\newglossaryentry{idt}{
	name=IDT,
	description={Interrupt Descriptor Table; a table which describes the
	address of the handler that should be run when a given interrupt or
	exception is triggered.}
}

\newglossaryentry{idtr}{
	name=IDTR,
	description={A register which holds the physical address of the \gls{idt}.}
}

\newglossaryentry{tr}{
	name=TR,
	description={Task Register; a register which is used as an index into the
	\gls{gdt} to find the \gls{tss}.}
}

\newglossaryentry{tss}{
	name=TSS,
	description={Task State Segment; a data structure which, among other
	things, determines the \gls{esp}-value used during a context switch
	triggered by an exception or interrupt.}
}

\newglossaryentry{pic}{
	name=PIC,
	description={Programmable Interrupt Controller; a hardware device which
	orders interrupts before delivering them to the processor}
}

\newglossaryentry{apic}{
	name=APIC,
	description={Advanced \gls{pic}.}
}

\newglossaryentry{lapic}{
	name=LAPIC,
	description={Local \gls{apic}. The LAPIC is the processor-local component
	of the \gls{apic}. Modern systems have one LAPIC per processor.}
}

\newglossaryentry{mmio}{
	name=MMIO,
	description={Memory-Mapped I/O. If you communicate with a device using
	MMIO, it means that the registers of the device are mapped into memory at
	some address, and so communication happens by reading from or writing to
	memory.}
}

\newglossaryentry{mpconfig}{
	name=mpconfig,
	description={mpconfig is a method for finding information about multiple
	processors described in Intel's Multi-processor specification.}
}

\newglossaryentry{bp}{
	name=BP,
	description={Bootstrap Processor; the first, and initially only, processor
	that runs when a system boots.}
}

\newglossaryentry{ap}{
	name=AP,
	description={Application Processor; any processor which is not a \gls{bp}.
	The APs only run once they are started by the \gls{bp}.}
}

\newglossaryentry{ipi}{
	name=IPI,
	description={Inter-Processor Interrupt. An interrupt sent by one processor
	to another using the \gls{lapic}.}
}

\newglossaryentry{pio}{
	name=PIO,
	description={Programmed Input/Output. A way to read and write data from/to
	disk (or another device) using instructions such as \texttt{inb} and
	\texttt{outb}. Often an alternative to \gls{mmio}.}
}

\newglossaryentry{ipc}{
	name=IPC,
	description={Inter-Process Communication. A mechanism which lets processes
	communicate.}
}

\newglossaryentry{pci}{
	name=PCI,
	description={Peripheral Component Interconnect. A type of bus to which
	devices, such as the network card, can be connected.}
}

\newglossaryentry{lfb}{
	name=LFB,
	description={Linear Frame Buffer, a buffer that repesents the pixels which
	are drawn to the screen.}
}

\makeglossaries



\title{
%\line(1,0){250}\\
\Large \bfseries
% TODO improve this title.
Operating System \\ Implementation
%\line(1,0){250}
}
\author{Thomas Hybel}
%TODO: have your email here.
\date{Aarhus University \\ November 2017}

\begin{document}
\pagenumbering{roman}
\maketitle

\begin{abstract} 
\noindent 
TODO: write an abstract here.
\end{abstract}
\newpage

\tableofcontents
\newpage
\pagenumbering{arabic}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Introduction}


- introduction
	- what are we trying to do
	- how are we doing it
		- following the course from MIT
	- why are we doing it
	- overview of the coming chapters

% TODO: write about our development environment, i.e., that we're using QEMU.

% TODO: write that you can use "git checkout lab1" to go back to the end of
% lab1, etc.
% TODO: say it's an exokernel, talk about what that means for our later
% design
% TODO: refer to the abbreviations list.

% TODO: read each of the sections' introductions and see whether they make
% sense when read alone in sequence
% TODO: read over the report watching for confusing terms, then add them to
% the glossary (e.g. "real mode")
% TODO: note that the goal was to learn, not to create the most user-friendly
% or efficient system. So we e.g. created our own boot loader initially
% instead of using GRUB, and we created a custom file system instead of
% implementing a common one.

% TODO rewrite "the goal of this lab was", it sounds bad and repetitive

% TODO: we haven't talked about ATA yet.

% TODO: did we describe the console subsystem which lets us do i/o initially?


% TODO: in the end we should browse through the kernel code and see if there's
% any of it we didn't describe anywhere

% TODO: we should have an appendix that describes how to get and run the code.

% TODO search for gls and remove as many as possible


%%%%% LAB 1 %%%%%


\chapter{Booting}
\label{sec:lab1}

In this lab we wrote initialization code and a boot loader for our kernel. 
The initialization code switches the processor to 32-bit protected mode. The
boot loader loads the rest of the kernel into memory and jumps to it.

\section{The boot process}
To understand the purpose of a boot loader, it helps to have an overview of
the startup process of an x86 machine.
When an x86 machine starts, its BIOS code runs. The BIOS initializes some of
the hardware, and then it loads the first sector (512 bytes) from the boot
medium into a hard-coded address which it jumps to.

This first sector will typically contain a small program known as the boot
loader. Its purpose is to load the main kernel from disk and transfer
execution to it.

Before the boot loader loads the kernel, the kernel should first run some
initialization code which sets up a more comfortable environment for the boot
loader and kernel to work in.


\section{Initialization code}
When the BIOS jumps into our code, the processor is running in 16-bit real
mode. However the code produced by a modern compiler expects to run in 32-bit
protected mode. The initialization code should therefore be written in
assembly and should switch processor modes.

The most important difference between real and protected mode is that real
mode does not support the use of a page table to implement virtual memory.

To switch to protected mode, a bit needs to be set in the control register
\gls{cr0}. We do so with the following assembly code:
\begin{verbatim}
# protected mode enable flag
.set CR0_PE_ON, 0x1 
movl    %cr0, %eax
orl     $CR0_PE_ON, %eax
movl    %eax, %cr0
\end{verbatim}

We do not immediately enable virtual memory, though, since we do not yet have
the infrastructure to set up a proper page table. This happens in section
\ref{sec:mem}. Until then, all addressing is physical.

To switch to 32-bit mode, we must update the \texttt{cs} (code segment)
register. The \texttt{cs} register is an offset into a table called the Global
Descriptor Table (\gls{gdt}) which is an array of descriptors. Each descriptor
describes a segment of memory; a bit in the descriptor determines whether the
segment contains 16-bit or 32-bit code. 

We use the \texttt{ljmp} instruction to update the \gls{cs} register and use a
32-bit segment descriptor. Next, the boot loader is executed; since the
processor is in 32-bit protected mode, the rest of the kernel code can be
written in C rather than assembly.

%The file \texttt{boot/boot.S} contains the code that performs the described
%tasks.



\section{The boot loader}
It is the task of the boot loader to load the main kernel and transfer
execution to it. Since the boot loader resides on the first sector of the hard
drive, it must load the kernel using no more than 512 bytes of code, minus the
bytes used by the initialization code.

Our kernel is compiled into an Execute and Linkable Format (ELF) file. The ELF
file specifies the physical addresses into which the code and data of the
kernel must be loaded. The boot loader must thus parse the ELF file to load
the kernel.

Once the boot loader has loaded the kernel, it determines the entry point
address from the ELF file and jumps there.


\section{Minimal kernel code}
At this point we were able to run kernel code. However we did not yet have any
functionality; unlike user-mode programs, the kernel does not have access to a
C standard library, unless we write one ourselves.

As a start, we wanted the ability to input and output text.
Our kernel uses the \texttt{inb} and \texttt{outb} instructions to
communicate with the outside world via a serial connection. We used this to
print a "hello, world" message and confirm that the kernel runs.

Using instructions such as \texttt{inb} and \texttt{outb} to communicate with
an input/output device is quite common. The method is known as Programmed
Input/Output (\gls{pio}); it will be used often in the following sections.


%%%%% LAB 2 %%%%%

\chapter{Memory management}
\label{sec:mem}
The goal of this lab was to enable virtual memory after setting up a page
table. To set up the page table, we first needed to implement a subsystem
for allocating and freeing pages of physical memory.


\section{Physical page management}
% sources: https://en.wikipedia.org/wiki/Nonvolatile_BIOS_memory
% http://wiki.osdev.org/CMOS
A given system has a limited amount of physical memory, depending on how much
RAM the machine has. This memory is split up into a number of pages. On x86 a
page is 4096 bytes, or \texttt{0x1000} in hexadecimal. Thus pages always aligned on
\texttt{0x1000}-byte boundaries. The kernel determines the amount of RAM by using
\gls{pio} to query a memory area called the CMOS. 

The physical page management subsystem will keep a reference count for each
page. If the count is zero, the page is free and can be allocated. This
metadata is stored in an array of \texttt{PageInfo} structs. Each entry in
this array directly corresponds to one physical page of memory, such that the
first \texttt{PageInfo} struct holds metadata about the first page of physical
memory, and so on.

The \texttt{PageInfo} of free pages are additionally stored in a linked list,
such that the kernel can return a free page in constant time.

We wrote the following functions to manage physical pages:
\begin{itemize}
\item \texttt{page\_alloc} is used to allocate a page of physical memory
\item \texttt{page\_free} is used to put a page on the free list
\item \texttt{page\_decref} and \texttt{page\_incref} are used to manage
reference counts of pages
\end{itemize}
These functions provide critical infrastructure needed by other kernel
features.



\section{Page table theory}
It is necessary to introduce some theory before we can explain how our kernel
initializes its page table.

The x86 page table is a two-level table whose main purpose is to let the
processor translate a virtual address to a physical address. The page table
can be thought of as a 1024-ary tree with two levels. 

A pointer to the first level of the page table can be found in the
\texttt{cr3} register. The first level is called the Page Directory. It
contains 1024 Page Directory Entries (\gls{pde}s). Each \gls{pde} points to a
second-level node, which contains 1024 Page Table Entries (\gls{pte}s). 
A \gls{pte} specifies a page of physical memory and its permissions, including
whether it is writable and whether it is accessible to user-mode code.

To translate from a virtual to a physical address, it is necessary to walk the
page table. Say that the process wishes to access a virtual address $v$. It
first looks in the \gls{cr3} register to find the Page Directory. It uses the
10 higher-order bits of $v$ to specify a \gls{pde}. It uses the next 10 bits
of $v$ to specify a \gls{pte}. The \gls{pte} contains the address of a
physical page. The last 12 bits of $v$ are used as an offset into this page,
and the address translation is complete. The processor also validates the
permissions of the page before the access, and generates a page fault if these
are inappropriate.

This is a costly process, and in practice the job is done by specialized
hardware called a Memory Management Unit (MMU). Additionally, recent
translations are cached in the so-called Translation Lookaside Buffer
(\gls{tlb}).


\section{Page table management}
\label{sec:pagetables}
We wrote the following functions to manage the page table:
\begin{itemize}
\item \texttt{pgdir\_walk} is called by most of the other functions to walk the
page table, finding the \gls{pte} corresponding to a given virtual address. It
allocates new levels of the page table as needed using \texttt{page\_alloc}
from the previous section.
\item \texttt{page\_insert} is used to insert a physical page into a page
table at a given virtual address. In other words, it finds a \gls{pte} using
\texttt{pgdir\_walk} and stores the physical address there.
\item \texttt{page\_lookup} finds the physical address of a page, given a
virtual address.
\item \texttt{page\_remove} invalidates a \gls{pte} in a page table.
\end{itemize}
To set up the page table, the kernel allocates pages of physical memory and
uses the newly implemented functions to insert these into the table.

The memory layout of the address space of the kernel is largely up to us.
Figure \ref{memlayout} gives a simplified overview of the layout we opted for.
\begin{figure}
    \centering
\begin{Verbatim}[fontsize=\small]
Virtual memory map:                                  Permissions
                                                     kernel/user
   4 Gig -------->  +------------------------------+
                    :              .               :
                    :              .               :
                    |------------------------------| RW/--
                    |                              | RW/--
                    |      Kernel code, data       | RW/--
                    |                              | RW/--
   KERNBASE, ---->  +------------------------------+ 0xf0000000      
   KSTACKTOP        |     CPU0's Kernel Stack      | RW/--  KSTKSIZE 
                    +------------------------------+                 
                    |     CPU1's Kernel Stack      | RW/--  KSTKSIZE 
                    +------------------------------+                 
                    :              .               :                 
                    :              .               :                 
                    +------------------------------+ 0xef800000
                    |  Cur. Page Table (User R-)   | R-/R-  PTSIZE
   UVPT      ---->  +------------------------------+ 0xef400000
                    |          RO PAGES            | R-/R-  PTSIZE
   UPAGES    ---->  +------------------------------+ 0xef000000
                    :              .               :                 
                    :              .               :                 
   USTACKTOP  --->  +------------------------------+ 0xeebfe000
                    |      Normal User Stack       | RW/RW  PGSIZE
                    :              .               :                 
                    :              .               :                 
                    +------------------------------+
                    :              .               :
                    :              .               :
                    +------------------------------+
                    |     Program code, data       |
   UTEXT -------->  +------------------------------+ 0x00800000
                    :              .               :                 
                    :              .               :                 
                    +------------------------------+ 0x00000000
\end{Verbatim}
    \caption{The virtual address space of the kernel}
    \label{memlayout}
\end{figure}
The diagram shows that the kernel code resides starting at virtual address
\texttt{0xf0000000}. The kernel stacks, used by processors when running
kernel-mode code, reside just below, between \texttt{0xefc00000} and
\texttt{0xf0000000}. Further down in the address space, between
\texttt{0xef400000} and \texttt{0xef800000}, we have the User Virtual Page
Table (UVPT) area, which gives user-mode processes read-only access to their
page table, enabling certain exokernel-style programs to work. The stack of
the user-mode program starts at \texttt{0xeebfe000} and grows towards lower
addresses. The user-mode code and data reside near the bottom of the address
space, around \texttt{0x00800000}.

We created a page table according to this layout. We then updated the
\texttt{cr3} register and set a bit in the \texttt{cr0} register to enable the
use of the page table. At this point our kernel had proper virtual memory.
Once user-mode processes have been implemented, virtual memory guarantees that
the processes cannot modify the address space of one another, nor can they
corrupt the kernel.


%%%%% LAB 3 %%%%%
% TODO: think of a better name for this lab, it's not quite right..
\chapter{Process management}

The goal of this lab was to run a user-mode process. This required us to write
infrastructure for managing processes and their metadata. We also modified our
kernel to handle any exceptions generated by user-mode processes. Finally we
implemented a system call mechanism to let processes interact with the kernel.


\section{Managing process metadata}
Each process has some associated information. This includes its state
(running, runnable, blocked, killed), its process ID, parent process ID, page
directory, saved register state, and so on. This metadata is stored in a
struct \texttt{Env}. The process subsystem is similar to the physical page
subsystem; an array holds the \texttt{Env} struct of each process on the
system, and free environments are stored in a linked list. We implemented
functions for creating, initializing, and destroying a process.

\section{ELF loading}
Programs are represented as ELF files. To launch a process, the kernel first
allocates a fresh page table. It then walks over each section in the ELF file,
reads at which virtual address the section should go, allocates corresponding
physical pages, inserts them into the page table, and copies the code or data
into the physical pages.

Note that we have not yet introduced a file system, so it is not immediately
clear where the kernel can find the programs which it should load. To solve
this problem we embed each user-mode program into the kernel as a blob of
binary data. In section \ref{sec:fs} we describe our implementation of a
proper file system for storing programs and data.

\section{Context switching}
Once a program has been loaded, the kernel must perform a context switch to
let the created process run. During a context switch the kernel restores the
saved general-purpose registers using the \texttt{popal} instruction. It also
updates \texttt{cr3} to switch to the new address space. Finally it uses the
\texttt{iret} instruction to restore the saved \texttt{eip}, \texttt{esp},
\texttt{eflags}, and \texttt{cs} registers. This transfers execution to
user-mode code. 

The kernel must also drop its privileges. The lower two bits of the
\texttt{cs} register determine the privilege level of the processor. This was
previously 0, since the kernel runs in ring 0. By setting this to 3 during the
\texttt{iret}, the processor switches to user-mode operation, which is ring 3.
Since instructions such as \texttt{iret} may only be run from privilege level
0, the process cannot simply modify its \texttt{cs} register to increase its
privileges.

At this point our kernel was able to successfully create a new process, load
program code and data into its address space, and let the process run.
Unfortunately the code had no way to give control back to the kernel, so it
simply ran forever, or at least until it triggered an exception or interrupt.
Since this was not yet handled, any exception caused the whole system to
crash.


\section{Theory of exceptions and interrupts}
While executing an instruction, the processor may trigger an exception. This
could e.g. happen due to a division by zero, or due to an illegal memory
access. When an exception occurs, the processor performs a context switch to
enter kernel mode. Then it runs some handler code specific to the exception.

The processor may also occasionally trigger an interrupt. This often happens
for asynchronous reasons; examples could be receiving a new network packet or
key press. Interrupts can also happen for synchronous reasons; for example, an
interrupt is raised as a result of executing the \texttt{int 0x80} instruction
which is commonly used for system calls. The behavior is the same for
exceptions; a context switch occurs to run a handler in kernel mode.

We now describe how the processor knows which code to run when an interrupt or
exception is raised. Exceptions and interrupts have numbers. Exceptions are
numbered from 0 to 31, and interrupts are numbered from 32 to 255.
% TODO: give some examples from http://wiki.osdev.org/Exceptions in a table.

These numbers specify an index into a table called the Interrupt Descriptor
Table (\gls{idt}). The physical address of the \gls{idt} is stored in the IDT
Register (\gls{idtr}), which can be read and set using the \texttt{lidt} and
\texttt{sidt} instructions, respectively.

The \gls{idt} table entry describes the address at which the handler for the
given interrupt resides.

As noted, an exception or interrupt triggers a context switch. Therefore the
processor needs to know on which stack it should save the registers of the
faulting process. To find this \gls{esp} value, the processor reads the Task
Register (\gls{tr}) which is an index into the \gls{gdt}. The \gls{gdt} entry
contains the address of a data structure called the Task State Segment
(\gls{tss}). The processor reads the new \gls{esp} value from the \gls{tss}.

To sum up: when an exception or interrupt occurs, the corresponding number is
looked up in the \gls{idt} to find the new \gls{eip} value. The \gls{tr},
\gls{gdt} and \gls{tss} are used to find the new \gls{esp} value. The
processor uses this information to store the registers of the faulting process
onto the new stack and execute the relevant handler code.


\section{Handling exceptions and interrupts}
We wrote a common function, \texttt{trap}, which is called whenever an
exception or interrupt occurs. For each exception and interrupt, we filled in
its \gls{idt} entry with a small stub which passes the number of the exception
or interrupt as the first argument in a call to \texttt{trap}.

The typical result of an exception is that the kernel terminates the running
process. However exceptions can also occur in kernel mode, in which case there
is a bug in the kernel. This results in a kernel panic, making the kernel
print the exception and hang.



\section{Handling system calls}
A process often needs to ask the kernel to perform some task for it. This
could e.g. involve printing text onto the console, reading a character from
the keyboard, or spawning a new child process. We needed to implement a
mechanism for triggering system calls in our kernel.

On the x86 architecture, the typical approach is to use the \texttt{int}
instruction, which triggers an interrupt when executed. On Linux, interrupt
\texttt{0x80} is used for system calls, but we are free to use any interrupt, so we
arbitrarily chose number \texttt{0x30}. This means that a program uses the \texttt{int
0x30} instruction to perform a system call. The interface was up to us, but we
kept it fairly standard; the \texttt{eax} register holds the system call
number, while arguments go in registers \texttt{ebx}, \texttt{ecx}, etc.

The \texttt{trap} function recognizes interrupt number \texttt{0x30} and calls the
\texttt{syscall} function, which uses a large \texttt{switch} to delegate each
system call to a specific handler.

We wrote system call handlers for input and output of a single character. At
this point we were able to run simple user-mode programs and have them
interact with the user. The programs could terminate themselves with a
specific system call, or by triggering any exception.


% TODO: also mention that our user-mode programs don't have much of a standard
% C library (yet)


%%%%% LAB 4 %%%%%
\chapter{Multiprocessing}
In the previous lab, we reached a point where our kernel could run a single
user-mode process. The current lab concerns itself with running multiple
processes concurrently, and having them interact. We implemented preemptive
multitasking, process forking, and inter-process communication. 
% TODO: this description misses a few of the following sections. We should
% update it to include more details.

Before we could do this, however, we needed to implement code for interacting
with a device called the LAPIC.


\section{Interacting with the LAPIC}
\label{sec:mpconfig}
% source: 
% http://wiki.osdev.org/APIC
% https://en.wikipedia.org/wiki/Advanced_Programmable_Interrupt_Controller
A Programmable Interrupt Controlled (\gls{pic}) is a device which is
responsible for managing interrupts for the processor. For example, if
multiple interrupts are generated simultaneously, the \gls{pic} can prioritize
the interrupts and deliver them one at a time. When Intel updated their
\gls{pic} standard to include new features, the conforming device was called
an Advanced PIC (\gls{apic}). The \gls{apic} has a component called the Local
APIC (\gls{lapic}) which is local to each processor. Thus modern systems have
one \gls{lapic} per processor.

In section \ref{sec:moreprocs} we need to query the \gls{lapic} to activate
more processors than just the initial one. Furthermore, in section
\ref{sec:preempt} we need to ask the \gls{lapic} to generate periodic timer
interrupts which our scheduler will use to preempt the running process.
Therefore it is relevant to describe how our kernel communicates with the
\gls{lapic}.

The processor can communicate with its \gls{lapic} using Memory-Mapped I/O
(\gls{mmio}). This means that the \gls{lapic} is mapped into memory at a
specific, system-dependent physical address. Reading at certain offsets will
correspond to reading from certain registers in the \gls{lapic}, and likewise
for writing.

This means that to communicate with the \gls{lapic}, the kernel merely needs
to read and write to certain addresses. The difficult part is figuring out the
physical address where the \gls{lapic} resides.

There are multiple ways to find the \gls{lapic}. For now, our kernel uses the
method described in Intel's multi-processor specification. We will refer to
this as the \gls{mpconfig} method.
% TODO: insert reference to specification..

We leave out the details for brevity, but the method boils down to the
following. The kernel searches for a structure called the MP floating pointer
structure. It does so by looking in certain parts of physical memory for the
string "\_MP\_", validating a checksum on the memory to ensure that this was
not a false positive. This structure points to a table called the MP
configuration table, which contains the physical address of the \gls{lapic}. 
% TODO: mention that we were given mpconfig code



\section{Activating more processors}
\label{sec:moreprocs}
So far we have been running our kernel on an emulated machine with a single
processor. A system with $n$ processors can be emulated by passing the
\texttt{-smp n} option to QEMU.

When a system with multiple processors boots, the hardware dynamically selects
one of the processors to be the Bootstrap Processor (\gls{bp}). The other
processors are called Application Processors (\gls{ap}s). At first, only the
\gls{bp} runs. It is up to the \gls{bp} to start up the remaining \gls{ap}s
once the system is ready.

Up to this point, only the \gls{bp} had been running our kernel so far;
\gls{ap}s were never activated. We therefore needed to write code which starts
the \gls{ap}s. This is done by asking the \gls{lapic} of the \gls{bp} to send
an Inter-Processor Interrupt (\gls{ipi}) to each of the \gls{ap}s. This
\gls{ipi} causes the \gls{ap}s to start executing code. The address of the
code to run is sent as part of the \gls{ipi}.

% TODO: glossary-ize "real mode"
The \gls{ap}s start in 16-bit real mode, just as the \gls{bp} did. They
therefore need to switch to 32-bit real mode mode. After doing so, each
\gls{ap} calls into the scheduler to run a new process.

% source:
% https://stackoverflow.com/questions/14261612/which-core-initializes-first-when-a-system-boots
% which refers to an intel manual

\section{Ensuring mutual exclusion}
With multiple processors running concurrently, all the typical issues of
concurrency arose. Specifically, multiple processors could modify internal
kernel structures simultaneously, leading to race conditions.

We prevented this in the trivial but inefficient way: we make sure that at
most one processor is running kernel code at the same time. In other words,
our kernel is not truly concurrent. Still, user-mode processes \emph{can} run
truly concurrently.

The "big kernel lock" is a spinlock. It is essentially a global variable which
determines whether the kernel is locked. A processor repeatedly uses the
\texttt{lock} and \texttt{xchg} instructions to atomically exchange the global
variable with the value 1. If the global value was zero, the processor now
holds the lock and may enter the kernel. Otherwise it must retry.

We added calls to lock and unlock the kernel in the right places. At this
point our kernel was capable of running multiple user-mode processes
concurrently. However these processes ran forever, so we needed a scheduler
next.


\section{Process scheduling}
\label{sec:preempt}
The scheduler has as its main responsibility to pick a new process to run
whenever the current process is scheduled out. We opted for round-robin
scheduling.

That is, the scheduler keeps a circular queue of all processes. To find the
next process, the scheduler retrieves the next process from the queue until it
finds one that is runnable, at which point the scheduler is done.

The scheduler also needs a way to preempt each process when its time slice
runs out. To accomplish this, during kernel initialization the kernel asks the
\gls{lapic} to raise a timer interrupt periodically, waiting some fixed amount
of bus cycles between each raised interrupt. Our \texttt{trap} function then
recognizes this timer interrupt and reacts by asking the scheduler to schedule
in a new process.

Our scheduler has much room for improval. Round-robin scheduling is not as
performant as more advanced algorithms. Additionally, it does not allow
adjustment of process priorities. Scheduling being a linear-time operation is
potentially worrying. 

A final issue is that the time slice assigned to each process is always of the
same duration. It would be useful to assign shorter time slices and more
frequent schedulings to processes which need to feel responsive (such as
graphics-based processes introduced later) and longer time slices to processes
that perform heavy computations.


\section{A simple fork mechanism}
So far, every process was directly spawned by the kernel. However, processes
should also be able to spawn more processes. We therefore needed to implement
a mechanism to let a process fork. In this section we describe our initial,
simple implementation of fork, and in the following section we improve it by
introducing a copy-on-write mechanism.

Since our kernel is an exokernel, we prefer
to do as much work as possible outside of kernel land. We therefore wrote a
number of system calls which can be combined to implement a user-mode fork.
Specifically, we wrote the following system calls:
\begin{itemize}
\item	\texttt{sys\_exofork}: creates a non-runnable child process with an empty address space.
\item	\texttt{sys\_env\_set\_status}: can mark a process as runnable.
\item	\texttt{sys\_page\_alloc}: allocates an empty page in the address space of a process.
\item	\texttt{sys\_page\_map}: maps a page from the current process into a child process.
\item	\texttt{sys\_page\_unmap}: unmaps a page from the current process or a child process.
\end{itemize}
Besides these system calls, it is also necessary for a process to have access
to information about the layout of its own address space. This is already the
case; in section \ref{sec:pagetables} on page table management, we set up the
page table of a process such that part of the address space contains the page
table itself.

To fork, a parent process goes through the following steps:
\begin{itemize}
\item The parent process calls \texttt{sys\_exofork} to create a new child
process with an empty address space. The child process is not initially
runnable.
\item The parent walks over its page table, and for each mapped page, it does
the following:
\begin{itemize}
\item The parent uses \texttt{sys\_page\_map} to create a temporary page at a
temporary address.
\item The parent copies the contents of the current page into the temporary
page.
\item The parent uses \texttt{sys\_page\_map} to insert the temporary page
into the address space of the child process at the original address.
\item The parent uses \texttt{sys\_page\_unmap} to remove the temporary page
from its own address space.
\end{itemize}
\item The parent marks the child as runnable using
\texttt{sys\_env\_set\_status}. 
\end{itemize}
At this point the fork is complete, and since the child is marked as runnable,
the scheduler will eventually schedule it in.

Note that for security reasons, the system calls are coded to ensure that a
process cannot modify pages in other, non-child processes.


\section{Copy-on-write fork}
The simple version of fork described in the previous section is slow and
memory-inefficient, because it indiscriminately copies every page of the
parent into the child process. This involves a lot of copying, and it means
that if the parent used $n$ physical pages of memory, then after a fork, $2n$
physical pages will be used.

However the same physical page can transparently be mapped into both the
parent and child process, as long as it is never modified. In fact, \emph{all}
the pages in the child process can initially be shared with the parent. It is
only once a write happens that a page must be copied. We have implemented such
a copy-on-write fork mechanism almost entirely in user land, following the
exokernel design philosophy.

To perform as much work in user land as possible, we implemented a mechanism
which lets a process handle its own page faults. By default, the kernel will
terminate the running process if a page fault occurs. However we have
implemented a system call, \texttt{sys\_env\_set\_pgfault\_upcall}, which lets
a process set a handler function. If a handler is set, the kernel will handle
a page fault by modifying the saved \gls{eip} and \gls{esp} registers of the
process, pushing the old register values onto an exception stack, and
switching the process back in.

Now, when a process forks, \emph{all} pages are shared between the parent and
child process. However writable pages have their writable bit removed from
their page table entry (\gls{pte}). Instead we set another bit which marks the
page as copy-on-write.

If either the child or parent process attempts to write to said page, a page
fault will occur, since the page is not writable anymore. The kernel delegates
to the registered user-mode handler function. The handler then uses the same
method as in the simple fork implementation to map a new writable page, copy
the contents of the old page onto it, and replace the copy-on-write page with
the new writable page.


\section{Inter-process communication}
Since our kernel is an exokernel, many of the features implemented in later
labs will reside in user land. Two examples are a file system daemon, and a
daemon implementing a network stack. Other processes need a way to interact
with these daemons to make use of their services. We therefore have need of an
inter-process communication (\gls{ipc}) mechanism.

We therefore implemented two system calls, \texttt{sys\_ipc\_recv} and
\texttt{sys\_ipc\_try\_send}, which enable \gls{ipc}. When a process calls 
\texttt{sys\_ipc\_recv}, it will hang waiting for a process to send it data.
\texttt{sys\_ipc\_try\_send} will send data to a process in a non-blocking
fashion. By default, a 32-bit integer is sent between processes, but for
efficiency an extra argument to the system calls allows the sender to share a
full page of memory with the recieving process.


This marks the end of the multiprocessing lab. Our kernel can now run
user-mode processes in a truly concurrent fashion. A scheduler manages the
running processes, preempting them when necessary. Processes can efficiently
fork and communicate via \gls{ipc}.


%%%%% LAB 5 %%%%%
\chapter{File system}
\label{sec:fs}
The goal of this lab was to implement a custom file system and a shell.

\section{Custom file system}
So far, our kernel has embedded any user-mode programs inside itself as binary
blobs. This is highly undesirable; it requires recompilation of the full
kernel to modify user-mode programs, and it makes it impossible for programs
to store data persistently on disk. We therefore implemented a custom file
system, which we now describe.

A file system exists on a disk. A disk can be thought of as a large amount of
writable space. We can partition the disk's space into blocks, where each
block has a specific size. For our file system, the block size will be 4096
bytes.

One of these blocks is special; it's called the "superblock". The superblock
holds any metadata needed for the file system, such as the disk size, where to
find the root folder, etc.

Our file system is laid out as follows. The disk is partitioned into blocks as
mentioned. Block 0, the first one, is not used by our FS. (This way it can
hold the boot loader.) Block 1 is, by convention, the superblock. 

The next blocks, starting at block 2, hold a bitmap of free blocks; bits
correspond to blocks on disk, and each bit is 1 if, and only if, the
corresponding block is free. This way we can allocate and free blocks.

The remaining blocks are used to store the concrete files and folders.

We have decided to keep our file system as simple as possible for ease of
implementation. Thus there is no concept of permissions, symbolic and hard
links, timestamps, and so on.

% TODO: include graphics from https://pdos.csail.mit.edu/6.828/2016/labs/lab5/disk.png

A file is represented in C as a \texttt{struct file}, which is stored in a
block. Such a \texttt{file} struct contains metadata, such as the file name
and size. The struct also has 10 pointers to the blocks that hold
the raw file data. If the data cannot fit in 10 blocks, the \texttt{file}
struct has a pointer to a block which holds another 1024 pointers to data
blocks. Thus our file system has a maximum file size of $(10+1024)*4096
= 4235264$ bytes, i.e., around 4 MB.

% TODO: also this https://pdos.csail.mit.edu/6.828/2016/labs/lab5/file.png

Folders are represented as a \texttt{struct folder}, which is exactly
identical to a \texttt{struct file}, except that the $10+1024$ block
pointers no longer point to raw data, but to other blocks holding
\texttt{file} or \texttt{folder} structs. There is a type flag allowing us to
distinguish between files and folders.

We used a small script to create a raw disk image with an initialized file
system of the described format. The script let us add files, such as sample
programs, to the file system. We then attached this raw disk to QEMU.


\section{File system daemon}
In accord with exokernel design, we let all file system interaction go through
a single privileged process which we call the file system daemon.

The daemon interacts with the disk using \gls{pio}. However normal user land
processes cannot use the \texttt{inb} and \texttt{outb} instructions to
perform \gls{pio}, so our kernel needs to give the daemon I/O privileges. It
does so by setting a bit in the \gls{eflags} register for the daemon process.

The file system daemon spends its time looping, waiting for other processes to
contact it via \gls{ipc}. Processes can send requests to open, read, write,
and stat files. The \gls{ipc} details are hidden behind our implementation of
a C standard library, such that processes have the usual interface with
functions such as \texttt{open}, \texttt{read} and \texttt{write}.

We have further improved the efficiency of the file system daemon by
implemented a block caching system. When a block is first read, its contents
are stored in RAM, and subsequent reads do not need to interact with the disk
until the cache entry is invalidated through a write.



To summarize, when a process wants to open a file, the following happens. The
process calls the \texttt{open} library function. The library uses \gls{ipc}
to contact the file system daemon. The daemon uses the file system superblock
to find the root folder. It walks over the folder structure by following block
pointers, until it finds the block that holds the data for the correct file.
This location on disk is stored in a file descriptor, and the number of the
file descriptor is returned to the first process via \gls{ipc}. When a
subsequent \texttt{read} is performed, the daemon uses the pointers in the
\texttt{file} struct to find the raw data block and sends the requested data
back via \gls{ipc}.



\section{Shell}
We implemented a simplistic shell which lets us load and execute programs from
disk. The shell also features input redirection ("<"), output redirection
(">"), and pipes ("|"). It is now possible to write to the disk and have the
changes persist across reboots.

% TODO: we didn't describe the init process.

% TODO: show an example interaction.


%%%%% LAB 6 %%%%%
\chapter{Networking}
The goal of this lab was to connect our kernel to the internet by writing a
network card driver.

\section{The e1000 network card}
The operating system is connected to network via a network card. The network
card has as its main responsibility to send and receive packets by interacting
with the physical layer, such as a wire.

The network card emulated by QEMU is the Intel e1000. We therefore need to
write a driver for this card. The task of the driver is to discover the
network card, initialize and enable it, drain incoming packets from the card,
and deliver outgoing packets to the card.

We found all the details needed for the driver in the manual. It describes in
detail how the card works, how it should be initialized, and so on.
%TODO: insert reference to the manual and a proper name..

The e1000 card is connected through \gls{pci}. Our kernel uses \gls{pio} to
scan the PCI bus, iterating over all connected devices. Each device has
numbers representings its class, subclass, vendor ID and device ID. The kernel
uses these numbers to recognize the e1000 card if it is connected. The
\gls{pci} interface lets us determine a physical address at which the e1000
card is mapped. 

This completes the discovery phase; the kernel can now use \gls{mmio} to
communicate with the card. Concretely, this means that the various registers
of the network card can be found at specific offsets from the base address
found via \gls{pci}. In other words, the registers of the card can be read
from and written to by simply reading or writing at certain memory addresses.

The next step is initializing the card. We next describe the data structures
which the card uses, since these are what need to be initialized.

\section{Packet queues}
Overall, the e1000 network card uses two circular queues for holding packets.

One queue, the transmit queue, is for packets which are yet to be sent. When
the kernel wants to transmit a packet, it places it into this queue. The e1000
periodically drains this queue and puts the packets on the wire.

The other queue, the receive queue, is for packets which the e1000 has
received but which have not yet been claimed by the kernel. Thus when a new
packet arrives on the wire, the e1000 picks it up and deposits it into the
receive queue. The kernel then periodically drains this queue.

Each queue is implemented as an array of descriptor structs. A descriptor
contains a physical address and a size, and thus it describes a memory area
which can hold one packet. Each queue also has a tail index register and a
head index register which are used during insertion and removal of packets.

This means that when a packet arrives from the network, the e1000 card finds
the descriptor at the tail of the receive queue, copies the raw packet data
into the described memory area, and advances the tail register. The kernel
takes a packet out of the queue by copying the raw packet out of the
descriptor at the head of the queue, and then incrementing the head register.
Transmission of packets is analogous.

This raises the question of what should happen if one of the queues is full.
According to the manual, if the receive queue is full, the e1000 card will
simply drop further packets. However if the transmit queue is full, it is up
to the kernel what should be done. The kernel could potentially store packets
until the network card has drained the queue. However for simplicity we have
decided to simply drop packets if the transmission queue is full. We are free
to do this since the protocols on higher levels of the network stack handle
packet loss.

\section{The e1000 driver}
Before the card can be enabled, our driver must initialize these queues. That
is, it must allocate the physical pages required to hold the queues
themselves. It must also allocate pages for the raw packet contents, and fill
out the addresses and lengths into the descriptors in the queues. The head and
tail registers of each queue must also be reset. 

Once our driver has done this, it writes into a register to enable the card.
From this point it is possible to transmit and receive packets. Our driver
implements the code for taking packets from the receive queue and inserting
packets into the transmit queue. User-mode programs have access to this driver
functionality through two system calls, \texttt{sys\_transmit} and
\texttt{sys\_receive}.

% TODO: we should reorder these subsections; we should first describe the card
% in a general sense (here is how to discover it, here is how it works with
% queues, etc.).. and then, in a separate section, we should describe what our
% driver does.

% TODO: lesson: writing drivers is tricky. (debugging: QEMU source modification)


\section{The network daemon}
With the driver written, our kernel was capable of transmitting and receiving
packets. However most processes only know of the data they wish to send; they
are not capable of constructing packets.

We therefore needed a TCP/IP stack. Allegedly, writing such a network stack
from scratch is an immense task. We therefore used the open-source stack lwIP
("lightweight IP"). lwIP acts as a black box for our purposes, taking raw data
as input, and producing packets as output.

Rather than adding lwIP to the kernel, we embedded it in a network daemon. The
daemon is responsible for managing sockets, just as the file system daemon was
responsible for managing file descriptors. The network daemon takes in
\texttt{send} requests via IPC, produces packets, and hands these over to the
operating system via the \texttt{sys\_transmit} system call. Likewise, it
takes in \texttt{receive} requests and uses \texttt{sys\_receive}, parses the
resulting packets, and hands over the data to the other process.

The IPC communication is hidden away in the C standard library, such that
user-mode programs have access to the usual \texttt{connect}, \texttt{send}
and \texttt{receive} interface.

% TODO: we have not written about MACs and ip address assignment..


\section{Web server}
To test the new functionality, we wrote a simple web server which can serve
files from the file system. The web server thus makes use of both the file
system daemon and the network daemon. 

We configured QEMU to forward requests at port 80 to the emulated machine. At
this point we were able to point a browser at the machine and be served a
working web page.






%%%%% GRAPHICS LAB %%%%%


\chapter{Graphics}
\label{sec:graphics}


% TODO write intro
intro goes here

\section{Drawing pixels}
Before our kernel could render a full GUI, it first needed the ability
to render a single pixel at a time. From this primitive is it straightforward
to implement rendering of bigger shapes like rectangles, lines, and so on.

To draw pixels, it is necessary to set a video mode. A video mode describes
the width, height, and depth of the screen. The video mode can be set by
querying the BIOS. Once the kernel selects a video mode, the BIOS maps a
buffer called the Linear Frame Buffer (\gls{lfb}) into RAM. 

The \gls{lfb} represents the pixels of the screen; it can be thought of as a
two-dimensional array, where each value is a 32-bit integer representing the
RGB value of a pixel on the screen. Thus to write a pixel to the screen, the
kernel must simply calculate the correct offset into the \gls{lfb} and write a
32-bit integer there.

To set the video mode, the BIOS can be queried with the \texttt{int 0x10}
instruction. This transfers execution to the code of the BIOS. However since
the BIOS is written as 16-bit real mode code, our kernel must switch the
processor back to 16-bit real mode before it can issue the interrupt.

We therefore wrote assembly code which switches the processor mode,
queries the BIOS to enumerate the valid video modes, and selects one with a
satisfactory resolution.

At this point our kernel was able to color the screen by writing all over the
\gls{lfb}. We were then ready to design the graphics stack.


\section{Graphics stack design}
To design our graphics stack, we first read up on how graphics work in common
operating systems and used this as a basis for our design. We decided to
have a central privileged process, called the display server, which is
responsible for most of the work.

The display server is responsible for spawning other graphical applications,
assigning a portion of the screen to each application. The display server is
the only process with access to the \gls{lfb}, so it is responsible for
drawing all pixels to the screen. 

Each graphical application is spawned by the display server. When this
happens, the two set up an area of shared memory through \gls{ipc}. This
memory is called the canvas of the application. The application can write
pixels into the canvas, and the display server will periodically read the
pixels from the canvas and write them to the \gls{lfb}.

Graphical applications wait in a so-called event loop; they continuously wait
for input events to arrive from the display server. When an input arrives, the
application handles it and can make changes to its canvas on this basis.

The kernel keeps a queue for input events. When raw input packets arrive from
the mouse or keyboard, a driver handles these packets by putting them into an
event queue. The display server periodically drains this queue through a
system call. It then forwards each event to the appropriate application.

A graphics library provides common functionality needed by the user
applications and the display server. The library provides functions for
rendering rectangles, straight lines, fonts, and windows.

The following diagram shows an overview of the different components and how
they interact with each other:
% TODO insert a diagram.


Thus imagine that a user sits in front of the machine with a terminal emulator
application open and active. If the user presses the 'A' key, the following
series of events will occur: 
\begin{itemize}
\item The user presses the 'A' key.
\item The keyboard generates an interrupt to signal to the kernel that input
is available.
\item The kernel keyboard driver reads the pressed key using \gls{pio}.
\item The driver puts the event into the events queue.
\item The display server eventually drains the events queue and finds the key
press event.
\item The display server finds that the terminal application is active and
therefore forwards the event to it using \gls{ipc}.
\item The terminal application receives the event and adds the 'A' to a buffer
which holds the current input of the user. This buffer will eventually be used
to run a command once the user presses the enter key.
\item The terminal application also wants to display the 'A' on the screen, so
that the user can see what is written so far. The application therefore asks
the graphics library to render an 'A' on its canvas using the default font.
\item The display server writes the pixels from the canvas into part of the
\gls{lfb}.
\item The user sees the 'A' appear on screen.
\end{itemize}


% TODO: tie these sections together better

\section{Details of the display server}
The display server needs to have write access to the \gls{lfb}, which is
mapped at a physical address by the BIOS. To accomplish this, the display
server uses a new system call, \texttt{sys\_map\_lfb}, which causes the kernel
to add the \gls{lfb} to the page table of the display server.
% TODO: rephrase this to "we could make the kernel write to LFB, but instead
% for effiency we map it directly via page tables"
% then the title of this can be "effiency"..

It is highly important that the display server is efficient; if it runs too
slowly, the system will have a low frame rate, and interaction will feel
choppy. We had to carefully optimize the code for the display server before we
got an acceptable frame rate inside QEMU.

One of the optimizations is the following. The display server takes the
content of each canvas and writes it into the \gls{lfb}. However if
canvases overlap, there is no need to first write the lower canvas into the
\gls{lfb}, and then immediately overwrite it with the canvas that is on top.
We therefore introduced a buffer held in RAM. The display server first
constructs the final canvas in this buffer, and then copies each pixel once
and for all into the \gls{lfb}.

% TODO: write about this: - uses simple data structures (linked lists)


\section{Sample graphical applications}
We have written two sample graphical applications. One is a simple terminal
emulator. The other is a simple paint program.

% TODO: maybe describe these in some more detail?

% TODO: fit in details about PS/2 mouse driver somewhere.


%%%%% HARDWARE LAB %%%%%
\chapter{Hardware}
Up to this point, our kernel had always been running in the QEMU emulator. In
this lab, our goal was to get it running on real hardware, specifically a
Packard Bell Dot S netbook. The road there was paved with surprisingly many
complications. Most of the issues were caused by the fact that QEMU emulates
different hardware than that of the netbook. There were also instances where
QEMU did not emulate certain aspects of a machine faithfully, causing latent
bugs to surface on real hardware.


\section{Booting from USB}
The build process of our kernel produces a raw disk image which QEMU will
boot. The first block of this image is the boot loader, which the BIOS loads
and transfers execution to as described in section \ref{sec:lab1}. Since QEMU
will boot from this image, we figured that so would the netbook. We put the
raw disk image on a USB drive and had the netbook boot from it. However the
netbook did not recognize the USB as a bootable medium.

It turns out that a USB drive must have a valid data structure called a Master
Boot Record (MBR) in its first block, otherwise most machines will not
recognize the drive as bootable. The MBR must also contain a valid data
structure called the BIOS Parameter Block (BPB).

The reason for this is historical; when USB technology was new, there was
disagreement on whether a USB drive should be a raw storage drive or a
bootable medium. Initially the user could decide through a BIOS setting. But
for usability reasons, manufacturers eventually instead used heuristics to
detect whether a USB is bootable or not. These heuristics are based on the
MBR and BPB.

Once we added a valid MBR and BPB to our boot loader, the netbook booted.
However the machine got stuck at some point during the boot loader code.


\section{No serial connection}
Previously the kernel printed debugging information through a serial
connection which QEMU emulated. However we did not have the hardware necessary
to set up a serial connection to the netbook. We therefore had no output,
which made it difficult to determine why the boot loader was hanging.

We therefore had to write code which outputs text to the screen instead.
Before a video mode has been set as described in section \ref{lab:graphics},
the machine is in text mode. In text mode, by convention a buffer at physical
address \texttt{0xb8000} represents the text on the screen; we can write characters
directly into this buffer to show text on screen.


\section{No USB driver}
With the ability to print text to the screen, we figured out why the boot
loader was hanging. Our boot loader is naively coded such that it loads the
kernel from a disk connected with an ATA connection. But currently the kernel
resides on a USB drive, which is a completely different interface.

To continue, we needed to write a USB disk driver for the boot loader. However
the boot loader is still constrained to 512 bytes, since it is loaded from the
first sector of disk by the BIOS. Fitting a USB driver there is tricky.

We therefore opted for a different approach. We booted from USB into a Linux
distribution, and used that to overwrite the hard drive of the netbook with
our raw kernel image. Now we can boot from a proper hard drive rather than
USB.

This approach had the major downside of being slow; we had to boot the weak
netbook into a Linux distribution and enter several commands manually every
time we wanted to test a new iteration of our kernel. This slowed development
speed down significantly.

Additionally, the boot loader continued to get stuck.


\section{No SATA support}
After more debugging we determined the problem: the hard drive in the netbook
uses a SATA connection, while the machine emulated by QEMU used ATA. Thus
reading the kernel from disk was failing.

Fortunately, ATA and SATA are almost identical. ATA uses \gls{pio} to
communicate with the disk on fixed ports. SATA uses the same protocol; the
only difference is that the ports are machine-specific and must be found with
\gls{pci}.

So we tried to add PCI support to our boot loader but ran out of space -- we
only had 512 bytes to work with, after all. As a temporary workaround, we
booted into a Linux distribution and used the "lspci" tool to figure out the
SATA I/O ports and hardcoded them. 

With that, the boot loader finally succeeded in loading the kernel, which
promptly broke; the scheduler was failing to switch in new applications.


\section{A better boot loader}
We attempted to debug the scheduler, but our development process was too slow
and unwieldy to get anywhere; as mentioned, every time we made a change to the
kernel code we had to boot into a Linux distribution and enter commands to
write the raw kernel image to the hard disk. We needed to get around this;
ideally we would simply insert a USB drive and immediately boot into our
kernel. 

To facilitate booting from USB, we replaced our boot loader with a better one.
GRUB is the gold standard for boot loaders; it supports booting from various
hard drive types, USB, and even booting via an ethernet connection. After
integrating GRUB into the project, we were finally able to boot directly from
USB, and development sped up significantly.

Our custom file system assumes that the boot loader will fit into the first
512 bytes, with the superblock residing in the following sector. However as
GRUB uses multiple stages, it needs much more space. We therefore had to
modify our file system such that the first 32 MB are reserved for GRUB, and
the superblock resides thereafter.


\section{Finding the LAPIC}
Usually the LAPIC generates clock interrupts periodically, and on such an
interrupt the scheduler switches in a new process. However no clock interrupts
were generated, and thus only one process got any processing time. 

In section \ref{sec:mpconfig} we described how our kernel uses the so-called
\gls{mpconfig} method to find the LAPIC; that is, it looks for an MP
configuration table in memory to find the physical address of the LAPIC. This
physical address is used to communicate with the LAPIC, asking it to generate
clock interrupts. However our kernel failed to find these MP configuration
tables. They were simply not present in the memory of the netbook.

It turns out that the \gls{mpconfig} method is outdated and unsupported by
modern hardware. We therefore had to implement a different way to find the
LAPIC.

We leave out the details of the method, but in essence newer machines contain
so-called ACPI tables. One of these is the APIC table, which contains the
physical addres of the LAPIC. The ACPI tables can be found by scanning a
region of memory for a data structure called the RSDP, which points at another
data structure, the RSDT, which finally points at the ACPI tables.

After implementing this finding and parsing of ACPI tables, the kernel found
the LAPIC and clock signals were generated, letting the scheduler work as
intended.

Then another bug was uncovered.


\section{A page table bug}
Our kernel was behaving oddly; modifying a \gls{pte} seemed to have no effect.
After the modification, writing to memory at the virtual address still
affected memory at the old physical address rather than the new one.

We figured that this must be related to the \gls{tlb}, since this seemed to be
a caching issue. After much searching we learned that the \texttt{invlpg}
instruction must be used to invalidate a \gls{tlb} entry after a \gls{pte} has
been modified. Otherwise the outdated cached \gls{tlb} entry will continue to
be used until it is evicted.

Interestingly, this bug never surfaced while running the kernel in QEMU. We
assume that QEMU does not faithfully emulate the TLB for performance
reasons.\footnote{VirtualBox behaved similarly to QEMU; we therefore believe
that this quirk can be used as a means to detect virtualization/emulation.}


\section{Broken mouse driver}
At this point the kernel successfully booted into a graphical interface.
However the mouse was behaving oddly, jumping around when moved. The PS/2
mouse driver we wrote during the graphics lab was at fault; the driver uses
\gls{pio} to read from the mouse. Before each \texttt{inb} instruction, it is
necessary to wait for the mouse to signal that it has sent another byte of
data. The driver did not do this. However in QEMU these waits were not
necessary; thus this was another instance of QEMU not emulating hardware
perfectly.


\section{The final result}
With all the changes and fixes described so far, the kernel finally ran on our
netbook.
% TODO: include a screenshot?




\chapter{Future work}
% TODO: we should talk about future work.
% - security: ASLR, probably many other things (it's not even a multi-user
% system yet)
% - capabilities?
% - basic programs like an editor, browser, etc.
% - better windowing system (faster algorithms, better fonts, z-coordinate..)
% - could get a better scheduler
% - could have true kernel concurrency
% - better file system (timestamps, ...)
% - more efficient display server
% - more drivers.... e.g. network cards.


\chapter{Conclusion}
- conclusion

% TODO: we could include screenshots to make things more concrete
% TODO: include (a link to) the source code somehow

% TODO: question: is it a good idea to have "mistakes made" and "lessons
% learned"? If so, how do we do this without it sounding too informal?



% TODO: add references (some links are in writeup txt files)


\begin{framed}
\makebox[\textwidth]{Lesson learned: foo} \\
% TODO:
Maybe we can use something like this to describe the various lessons we
learned...
\end{framed}



\begin{thebibliography}{1}

% use like this: \cite{sigar1, sigar2}

% \bibitem{proc}
% Procyon,
% \\\texttt{http://bitbucket.org/mstrobel/procyon}
% 
% \bibitem{em} 
% Exam Monitor website.
% \\\texttt{http://exammonitor.dk}
% 
% \bibitem{jnlp}
% Exam Monitor JNLP file.
% \\\texttt{http://login.exammonitor.dk/exam.jnlp}
% 
% \bibitem{sigar1}
% Hyperic SIGAR website.
% \\\texttt{https://support.hyperic.com/display/SIGAR/Home}
% 
% \bibitem{sigar2}
% libsigar at GitHub.
% \\\texttt{https://github.com/hyperic/sigar}
% 
% \bibitem{sigarapache}
% The license file of SIGAR.
% \\\texttt{https://github.com/hyperic/sigar/blob/master/LICENSE}
% 
% \bibitem{agents}
% The package java.lang.instrument.
% \\\texttt{https://docs.oracle.com/javase/7/docs/api/java/lang/instrument/package-summary.html}
% 
% \bibitem{javassist}
% Javassist website.
% \\\texttt{http://www.javassist.org}
% 
% \bibitem{openjdk}
% OpenJDK website.
% \\\texttt{http://openjdk.java.net/}
% 
% \bibitem{webportal}
% Exam Monitor web portal.
% \\\texttt{https://sdu.exammonitor.dk/myexams.php}
% 
% \bibitem{x86harmful} 
% Joanna Rutkowska. Intel x86 considered harmful. 2015.
%  
% \bibitem{sokintro} 
% Bhushan Jain, Mirza Basim Baig, Dongli Zhang, Donald E. Porter, and Radu Sion.
% SoK: Introspections on Trust and the Semantic Gap. 2014.
% 
% \bibitem{appshield} 
% Yueqiang Cheng, Xuhua Ding, Robert H. Deng. AppShield: Protecting Applications
% against Untrusted Operating System. 2013.
% 
% \bibitem{towardsappsec} 
% Dan R. K. Ports, Tal Garfinkel. Towards Application Security on Untrusted
% Operating Systems. 2008.
% 
% \bibitem{haven} 
% Andrew Baumann, Marcus Peinado, and Galen Hunt. Shielding Applications from an
% Untrusted Cloud with Haven. In \emph{Proceedings of the 11th USENIX Symposium
% on Operating Systems Design and Implementation.} 2014.
% 
% \bibitem{sokhiee} 
% Fengwei Zhang, Hongwei Zhang. SoK: A Study of Using Hardware-assisted Isolated
% Execution Environments for Security. In \emph{Hardware and Architectural
% Support for Security and Privacy (HASP '16)}. 2016.
% 
% \bibitem{sgxexplained} 
% Victor Costan and Srinivas Devadas. Intel SGX Explained. 2016.
% 
% \bibitem{malwareinenclaves} 
% Jeroen van Prooijen. The Design of Malware on Modern Hardware: Malware Inside
% Intel SGX Enclaves. 2016.
% 
% \bibitem{enclavesinpractice} 
% JP Aumasson, Luis Merino. SGX Secure Enclaves in Practice: Security and Crypto
% Review. Presented at \emph{Black Hat USA 2016}.
% 
% \bibitem{opensgx} 
% Prerit Jain, Soham Desai, Seongmin Kim, Ming-Wei Shih, JaeHyuk Lee,
% Changho Choi, Youjung Shin, Taesoo Kim, Brent Byunghoon Kang, Dongsu Han.
% OpenSGX: An Open Platform for SGX Research. 2016.
% 
% \bibitem{iago} 
% Stephen Checkoway, Hovav Shacham. Iago Attacks: Why the System Call
% API is a Bad Untrusted RPC Interface. 2013.

\end{thebibliography}

\printglossary[title=Abbreviations]



\end{document}

