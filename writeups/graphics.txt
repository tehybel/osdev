

Time: 13-15, 13-15:15, 21:30-00, 11:30-14:30, 

The next task is figuring out how to get graphics working. I'll keep some
unpolished notes here.

Some links:

	https://web.archive.org/web/20150725135958/http://web.mit.edu/amdragon/www/pubs/sqrtx-6.828.html
	http://wiki.osdev.org/Graphics_stack
	http://wiki.osdev.org/VESA_Video_Modes 
	http://wiki.osdev.org/User:Omarrx024/VESA_Tutorial
	http://wiki.osdev.org/VGA_Hardware
	http://wiki.osdev.org/How_do_I_set_a_graphics_mode
	http://wiki.osdev.org/GUI
	https://en.wikipedia.org/wiki/VESA_BIOS_Extensions
	https://wiki.gentoo.org/wiki/QEMU/Options#Graphics_card


VESA is apparently a standard that lets you interface uniformly with a number
of different graphics cards. So we just need to write a VESA driver.

First, though, I need an overview of how graphics are actually drawn on the
screen. I have no clue yet how it all works.

Maybe we want VGA rather than VESA; according to a forum thread, it's the
easier option to start with.

	Many video cards have two interfaces, one VGA interface for low
	resolutions, and the VESA VBE interface for higher resolutions.
	Alternatively, you can write your own code to directly deal with the
	graphics hardware.
		- http://wiki.osdev.org/How_do_I_set_a_graphics_mode


What I'm really missing at the moment is an overview of how graphics work. I
have *no clue* how these colored pixels are showing up on my screen.

This page gives a bit of an overview:

	http://wiki.osdev.org/Graphics_stack

Apparently the device driver interacts with the graphics card and can only do
very simple things like "draw a pixel here". Instead of writing a driver for
every card, you instead write a driver for the VESA or VGA interface, which
most cards support. VGA is simpler, but lower resolution, than VESA.

Once you can draw pixels, you can create a stack on top: a library to draw
lines and rectangles, and then you can draw "widgets" like windows and
buttons. The window manager uses these widget-drawing functions and manages
where windows are placed on screen.

We're also going to need a driver for a USB mouse.

Here are more details on how to actually implement this from the bottom up:

	http://wiki.osdev.org/GUI



According to that page:

	The tutorial assumes ... the video resolution has been set using VBE in a
	linear frame buffer
	
	The kernel should probe the BIOS for the VBE Mode Info Block (see Getting
	VBE Mode Info), which returns information about the established video
	resolution mode.

So let's figure out how to do that.


According to this page:

	http://wiki.osdev.org/Getting_VBE_Mode_Info

We need to use int 0x10. But that's not available in protected mode.

So we need to call it from real mode, before we switch to protected mode.

We need to use function 0x4F02, with a value of 0xC118 (?) to create a linear
frame buffer, then use 0x4F01 to get the physical address of the LFB. More
details here:

	http://wiki.osdev.org/Getting_VBE_Mode_Info

But to use 0x4F02 ("Set Video Mode") we first need to pick a mode.

The example code on that page uses virtual86 interrupts. What's that?

According to this page:

	http://wiki.osdev.org/Virtual_8086_Mode

It's a way to emulate real mode from within protected mode. You set a flag in
the EFLAGS register, and then you're emulating real mode.

It seems that we need the ability to run v86 (virtual-8086) processes, because
then we can simply spawn a process which switches video modes for us.

So that should be the first step.

How do you enter v86? By setting a flag in the eflags register before doing an
iret. So that should be doable..

I've added a mode that does this, but now the created process segfaults with a
page fault upon the first instruction at 0x20. Why is it at 0x20? The entry
point is 0x800020. I guess registers are 16-bit now, then.

How do we compile for v86? Turns out that gcc can't do that. We need an 8086
compiler. But I think we're going off track, now.

Now that I think about it, we just need to be able to perform the int 0x10
instruction, nothing else. So we need a "v86int" function.

To switch to and from v86 mode, we can have syscalls that do this.

But actually this is going to be really hard. We need access to physical
addresses inside the function which sets up modes. We only have that from the
kernel.

It would be far easier to do all this work directly in the early kernel code,
rather than having to switch to v86 mode.



So let's try to do that instead.


...

Nope, that's really hard, too.

I guess we do want to be able to switch to v86 after all.



So I propose the following syscall:

	sys_v86

which switches to virtual-8086 mode by setting the flag in eflags. It will
then set $ip=0, $sp=0x1000, and go from there. So the program must first have
copied the right code into address 0, and have mapped 0x1000, too.

How does the program "get back"? The OS can remember the next instruction's
address. When the program is done running in v86 mode, it will do..
something.. and then the OS restores its state.

What will the program do to signal that it wants to return to normal mode? It
could issue a breakpoint instruction.

That should work. Maybe.


........

This turns out to be a total mess. Switching to virtual-8086 mode isn't easy.
And then, interrupts are handled in a very complicated manner. We can't just
do an "int 0x10", because that will trigger a GPF, going via the IDT. There
should be a way to make the interrupt work as it would in real mode, but it's
complicated.


http://f.osdev.org/viewtopic.php?f=1&t=11108




.......


To get graphics, we need to set the video mode using int 0x10 for the VESA
interface. To use int 0x10, we need to (1) be in real mode, or (2) be in v86
mode. 

Option (1) is infeasible, because we switch to protected mode very early, in
the first 512 bytes loaded by the BIOS from the first sector of the HDD. We
already use around 400 of those 512 bytes, so we'd have to switch video modes
using very little code. Alternatively we could switch back to real mode from
protected mode, but doing so is rather complicated.

So we go for option (2). We already have a syscall which switches to v86 mode.
Unfortunately, if we then do an int 0x10, the interrupt goes through the IDT
instead of issuing the BIOS interrupt which I expected.

So to continue we must understand how interrupts work in v86 mode. Exactly
which changes must we make to issue interrupts directly to the BIOS?

One way to figure this out is to read the Intel manual; first the simplified
version, then the full manual.

Another way is to find source code that does this and understand it. That's
what I'll try first, since it would be much easier.

Let's look for options:
- minoca OS: this is the most horrible code I've ever seen. Anyway, they
  switch back to real mode from protected mode. It's indeed complicated,
  taking lots of assembly instructions to set up GDT, switch to 16-bit mode,
  etc.
- Acess2: they've.. written an emulator? I have no clue what's going on. Ugly
  code.
- AQUA: this relies on GRUB doing the work for it
- ominos: has the ability to spawn a process directly in v86 mode. I don't
  understand why their "int 0x10" just works.

More OSes here: http://wiki.osdev.org/Projects

This isn't helping much. I'll try to read the manual instead..

So now the question to answer is: what happens when I issue an interrupt in
v86 mode?

From Intel manual:

	The processor services a software interrupt generated by code executing in
	the virtual-8086 task (such as a software interrupt to call a MS-DOS*
	operating system routine). The processor provides several methods of
	handling these software interrupts, which are discussed in detail in
	Section 20.3.3, “Class 3—Software Interrupt Handling in Virtual-8086
	Mode”. Most of them involve the processor entering protected mode, often
	by means of a general-protection (#GP) exception.

	IA-32 processors that incorporate the virtual mode extension (enabled with
	the VME flag in control register CR4) are capable of redirecting
	software-generated interrupts back to the program’s interrupt handlers
	without leaving virtual-8086 mode. See Section 20.3.3.4, “Method 5:
	Software Interrupt Handling”, for more information on this mechanism.

Something else to be aware of:

	The CPL is always 3 while running in virtual-8086 mode; if the IOPL is
	less than 3, an attempt to use the IOPL-sensi- tive instructions listed
	above triggers a general-protection exception (#GP). 

Notes from section 20.3:

- Apparently interrupt handlers are meant to be at address 0.
- We might have to set the VME bit in CR4
- The IOPL field of eflags may matter
- The software interrupt redirection bit map in the TSS may matter

	When the processor receives a software interrupt (an interrupt generated
	with the INT n instruction) while in virtual-8086 mode, it can use any of
	six different methods to handle the interrupt.
	
	The method selected depends on the settings of the VME flag in control
	register CR4, the IOPL field in the EFLAGS register, and the software
	inter- rupt redirection bit map in the TSS.

According to table 20-2, we need to set VME=1, IOPL=3, bitmap_bit=0. This will
result in method 5, "Interrupt redirected to 8086 program interrupt handler"

I've set the bitmap bit and the VME flag, and I'm still getting a GPF. Could
it be the IOPL? IOPL is bits 12+13 in the eflags register, which is
0x00020292.  

	>>> 0x00020292 & 0x00003000
	0

Yeah, it should be 3. Now I set it to 3, but still I get a GPF..

Let's try to move to another page than 0...


The problem is that I don't understand why I'm getting a GPF. Let's google..

Aha. If I set the privilege level of entry 0x10 in the IDT such that user-mode
processes are allowed to do an "int 0x10", I no longer get a GPF -- I get a
FPE. But this shouldn't be happening at all, since we're supposed to be in
8086 mode.

It almost seems as though the bitmap isn't properly set up.. But it seems to
be.


Let's try to make sure that address 0 is mapped.... that didn't help.

It almost seems as though v86 extensions isn't enabled. But I did set the bit
in CR4...

This page could be useful:

	http://wiki.osdev.org/Virtual_Monitor

Perhaps we can have the GPF-handler redirect execution back to the v86
process?

From that page:

	VME aren't available on QEMU, though

Can we confirm this somehow? Yes, with CPUID.

...

Oh. QEMU really doesn't support VME. So that's why I'm not seeing the expected
behavior. I should have checked for that right away...

That also explains why the guys who implemented this used Bochs instead of
QEMU...












- others must have done this already. Can I find source somewhere to read?

- 


