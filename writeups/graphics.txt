

Time: 13-15, 13-15:15, 21:30-00, 11:30-14:30, 12-12:30, 15:20-17:40,
17:50-19:50, 10-11:45, 19:30-20, 11:30-TODO

The next task is figuring out how to get graphics working. I'll keep some
unpolished notes here.

Some links:

	https://web.archive.org/web/20150725135958/http://web.mit.edu/amdragon/www/pubs/sqrtx-6.828.html
	http://wiki.osdev.org/Graphics_stack
	http://wiki.osdev.org/VESA_Video_Modes 
	http://wiki.osdev.org/User:Omarrx024/VESA_Tutorial
	http://wiki.osdev.org/VGA_Hardware
	http://wiki.osdev.org/How_do_I_set_a_graphics_mode
	http://wiki.osdev.org/GUI
	https://en.wikipedia.org/wiki/VESA_BIOS_Extensions
	https://wiki.gentoo.org/wiki/QEMU/Options#Graphics_card


VESA is apparently a standard that lets you interface uniformly with a number
of different graphics cards. So we just need to write a VESA driver.

First, though, I need an overview of how graphics are actually drawn on the
screen. I have no clue yet how it all works.

Maybe we want VGA rather than VESA; according to a forum thread, it's the
easier option to start with.

	Many video cards have two interfaces, one VGA interface for low
	resolutions, and the VESA VBE interface for higher resolutions.
	Alternatively, you can write your own code to directly deal with the
	graphics hardware.
		- http://wiki.osdev.org/How_do_I_set_a_graphics_mode


What I'm really missing at the moment is an overview of how graphics work. I
have *no clue* how these colored pixels are showing up on my screen.

This page gives a bit of an overview:

	http://wiki.osdev.org/Graphics_stack

Apparently the device driver interacts with the graphics card and can only do
very simple things like "draw a pixel here". Instead of writing a driver for
every card, you instead write a driver for the VESA or VGA interface, which
most cards support. VGA is simpler, but lower resolution, than VESA.

Once you can draw pixels, you can create a stack on top: a library to draw
lines and rectangles, and then you can draw "widgets" like windows and
buttons. The window manager uses these widget-drawing functions and manages
where windows are placed on screen.

We're also going to need a driver for a USB mouse.

Here are more details on how to actually implement this from the bottom up:

	http://wiki.osdev.org/GUI



According to that page:

	The tutorial assumes ... the video resolution has been set using VBE in a
	linear frame buffer
	
	The kernel should probe the BIOS for the VBE Mode Info Block (see Getting
	VBE Mode Info), which returns information about the established video
	resolution mode.

So let's figure out how to do that.


According to this page:

	http://wiki.osdev.org/Getting_VBE_Mode_Info

We need to use int 0x10. But that's not available in protected mode.

So we need to call it from real mode, before we switch to protected mode.

We need to use function 0x4F02, with a value of 0xC118 (?) to create a linear
frame buffer, then use 0x4F01 to get the physical address of the LFB. More
details here:

	http://wiki.osdev.org/Getting_VBE_Mode_Info

But to use 0x4F02 ("Set Video Mode") we first need to pick a mode.

The example code on that page uses virtual86 interrupts. What's that?

According to this page:

	http://wiki.osdev.org/Virtual_8086_Mode

It's a way to emulate real mode from within protected mode. You set a flag in
the EFLAGS register, and then you're emulating real mode.

It seems that we need the ability to run v86 (virtual-8086) processes, because
then we can simply spawn a process which switches video modes for us.

So that should be the first step.

How do you enter v86? By setting a flag in the eflags register before doing an
iret. So that should be doable..

I've added a mode that does this, but now the created process segfaults with a
page fault upon the first instruction at 0x20. Why is it at 0x20? The entry
point is 0x800020. I guess registers are 16-bit now, then.

How do we compile for v86? Turns out that gcc can't do that. We need an 8086
compiler. But I think we're going off track, now.

Now that I think about it, we just need to be able to perform the int 0x10
instruction, nothing else. So we need a "v86int" function.

To switch to and from v86 mode, we can have syscalls that do this.

But actually this is going to be really hard. We need access to physical
addresses inside the function which sets up modes. We only have that from the
kernel.

It would be far easier to do all this work directly in the early kernel code,
rather than having to switch to v86 mode.



So let's try to do that instead.


...

Nope, that's really hard, too.

I guess we do want to be able to switch to v86 after all.



So I propose the following syscall:

	sys_v86

which switches to virtual-8086 mode by setting the flag in eflags. It will
then set $ip=0, $sp=0x1000, and go from there. So the program must first have
copied the right code into address 0, and have mapped 0x1000, too.

How does the program "get back"? The OS can remember the next instruction's
address. When the program is done running in v86 mode, it will do..
something.. and then the OS restores its state.

What will the program do to signal that it wants to return to normal mode? It
could issue a breakpoint instruction.

That should work. Maybe.


........

This turns out to be a total mess. Switching to virtual-8086 mode isn't easy.
And then, interrupts are handled in a very complicated manner. We can't just
do an "int 0x10", because that will trigger a GPF, going via the IDT. There
should be a way to make the interrupt work as it would in real mode, but it's
complicated.


http://f.osdev.org/viewtopic.php?f=1&t=11108




.......


To get graphics, we need to set the video mode using int 0x10 for the VESA
interface. To use int 0x10, we need to (1) be in real mode, or (2) be in v86
mode. 

Option (1) is infeasible, because we switch to protected mode very early, in
the first 512 bytes loaded by the BIOS from the first sector of the HDD. We
already use around 400 of those 512 bytes, so we'd have to switch video modes
using very little code. Alternatively we could switch back to real mode from
protected mode, but doing so is rather complicated.

So we go for option (2). We already have a syscall which switches to v86 mode.
Unfortunately, if we then do an int 0x10, the interrupt goes through the IDT
instead of issuing the BIOS interrupt which I expected.

So to continue we must understand how interrupts work in v86 mode. Exactly
which changes must we make to issue interrupts directly to the BIOS?

One way to figure this out is to read the Intel manual; first the simplified
version, then the full manual.

Another way is to find source code that does this and understand it. That's
what I'll try first, since it would be much easier.

Let's look for options:
- minoca OS: this is the most horrible code I've ever seen. Anyway, they
  switch back to real mode from protected mode. It's indeed complicated,
  taking lots of assembly instructions to set up GDT, switch to 16-bit mode,
  etc.
- Acess2: they've.. written an emulator? I have no clue what's going on. Ugly
  code.
- AQUA: this relies on GRUB doing the work for it
- ominos: has the ability to spawn a process directly in v86 mode. I don't
  understand why their "int 0x10" just works.

More OSes here: http://wiki.osdev.org/Projects

This isn't helping much. I'll try to read the manual instead..

So now the question to answer is: what happens when I issue an interrupt in
v86 mode?

From Intel manual:

	The processor services a software interrupt generated by code executing in
	the virtual-8086 task (such as a software interrupt to call a MS-DOS*
	operating system routine). The processor provides several methods of
	handling these software interrupts, which are discussed in detail in
	Section 20.3.3, “Class 3—Software Interrupt Handling in Virtual-8086
	Mode”. Most of them involve the processor entering protected mode, often
	by means of a general-protection (#GP) exception.

	IA-32 processors that incorporate the virtual mode extension (enabled with
	the VME flag in control register CR4) are capable of redirecting
	software-generated interrupts back to the program’s interrupt handlers
	without leaving virtual-8086 mode. See Section 20.3.3.4, “Method 5:
	Software Interrupt Handling”, for more information on this mechanism.

Something else to be aware of:

	The CPL is always 3 while running in virtual-8086 mode; if the IOPL is
	less than 3, an attempt to use the IOPL-sensi- tive instructions listed
	above triggers a general-protection exception (#GP). 

Notes from section 20.3:

- Apparently interrupt handlers are meant to be at address 0.
- We might have to set the VME bit in CR4
- The IOPL field of eflags may matter
- The software interrupt redirection bit map in the TSS may matter

	When the processor receives a software interrupt (an interrupt generated
	with the INT n instruction) while in virtual-8086 mode, it can use any of
	six different methods to handle the interrupt.
	
	The method selected depends on the settings of the VME flag in control
	register CR4, the IOPL field in the EFLAGS register, and the software
	inter- rupt redirection bit map in the TSS.

According to table 20-2, we need to set VME=1, IOPL=3, bitmap_bit=0. This will
result in method 5, "Interrupt redirected to 8086 program interrupt handler"

I've set the bitmap bit and the VME flag, and I'm still getting a GPF. Could
it be the IOPL? IOPL is bits 12+13 in the eflags register, which is
0x00020292.  

	>>> 0x00020292 & 0x00003000
	0

Yeah, it should be 3. Now I set it to 3, but still I get a GPF..

Let's try to move to another page than 0...


The problem is that I don't understand why I'm getting a GPF. Let's google..

Aha. If I set the privilege level of entry 0x10 in the IDT such that user-mode
processes are allowed to do an "int 0x10", I no longer get a GPF -- I get a
FPE. But this shouldn't be happening at all, since we're supposed to be in
8086 mode.

It almost seems as though the bitmap isn't properly set up.. But it seems to
be.


Let's try to make sure that address 0 is mapped.... that didn't help.

It almost seems as though v86 extensions isn't enabled. But I did set the bit
in CR4...

This page could be useful:

	http://wiki.osdev.org/Virtual_Monitor

Perhaps we can have the GPF-handler redirect execution back to the v86
process?

From that page:

	VME aren't available on QEMU, though

Can we confirm this somehow? Yes, with CPUID.

...

Oh. QEMU really doesn't support VME. So that's why I'm not seeing the expected
behavior. I should have checked for that right away...

That also explains why the guys who implemented this used Bochs instead of
QEMU...



............

What options do we have now?
- keep using qemu, switch to real mode for int 0x10
- switch to bochs or such, use v86 with VME

I'll go with the former option. 



............

We now successfully switch back to real mode to have an LFB allocated.

Now what? How does this LFB work? What do I write into it?


I figured it out with the help of:

	http://wiki.osdev.org/GUI
	http://wiki.osdev.org/User:Omarrx024/VESA_Tutorial



...........


Now we can draw pixels and rectangles. I have no overview of what should
happen next. What are we still missing?

- a memory allocator (malloc, free, realloc etc.)
- window system, including:
	- ability to render fonts
	- functions to draw widgets, e.g. windows, buttons etc.
	- window server
	- communications protocol
- terminal emulator
- mouse driver


I currently have no idea how a window manager / x server / etc. works. That's
probably the main thing to figure out next.

It seems like we'll have one process that's a "graphics server" which will be
responsible for drawing to the LFB. But then how do other processes tell it
how their window looks? And what about mouse and keyboard input? Should that
go directly to the graphics process? And then it can send the input on to the
other processes?

I need to find some resource that explains all this..

This page explains some of it, but not everything:

	http://wiki.osdev.org/GUI


I guess the graphics server will keep track of all the different windows. When
an application wants to paint a window, it'll have to delegate to the graphics
server.

Then the graphics server can keep track of which window is selected, and if we
press a key it can send the input to the relevant process. Or the process can
listen for input. I'm not sure how that part should work yet.

Each process can have a "default look" with a window, an X button to close the
application, etc.

What about mouse presses? And graphics? How will an application tell the
graphics server how it looks?

An application could tell the graphics server, "here, draw this object for me.
And if anyone clicks it, call this callback." That would be one way. But I'm
making this up as I go; I should find a resource describing the right way.


I guess we'll give each process a canvas, if it asks for it. The canvas is
defined as what's inside a window. The process can then do whatever it wants
to the canvas, and can ask to have it drawn by the graphics server. Whenever
an event happens, we will deliver it to the appropriate application. Then it's
up to the application to decide what to do with it. So we could send like:

	event(mouse_click, x=100, y=124)
	event(key_press, key="A")

I'll look for more resources..


It turns out that what I'm coming up with is called a windowing system:

	https://en.wikipedia.org/wiki/Windowing_system

The "graphics server", as I called it, is also called a display server, window
server or compositor. For now let's call it a window server.


So a window system is a full "package" that provides a window server, as well
as primitives for e.g. rendering fonts, drawing windows, rectangles, etc.

This rendering is done by a window server. The server has clients; those are
the GUI applications. There is a protocol for communication between the
clients and the server. Thus an application can say "draw this text at this
position with this font" and the window server does the actual work.

In this terminology, on Unix there's the X window system, which has the X
server which I've heard of.

More info here:

	https://en.wikipedia.org/wiki/Display_server

The linux program "xev" displays all events it gets from the X server. That
could be useful for inspiration.


To proceed, we need to define a protocol used between the window server and
clients. We also need to figure out how this communication will happen.

We can create some subgoals:
- handle mouse events, passing them to the graphics server
- make the graphics server display the mouse
- make the graphics server receive all keyboard input
- define a protocol between server and clients


We can start on the first subgoal already, getting mouse input to work.

But I'd like to think about the protocol already. One thing that isn't clear
to me yet is this: the server is notified by the OS when keyboard/mouse input
is ready (but how?). Then this information must flow to the client
applications (but how?). If an application wants to draw its canvas, it must
tell the server this (but how?). 

First question: how does input (mouse click, keyboard press, etc.) reach the
window server? Currently an application can run getc() or such to grab input
from the OS, while the OS has stored the input in some buffer. We could have
the window server periodically drain this queue, but that seems slow. I don't
see a better option, though, so okay. We'll need a syscall to drain the mouse
and keyboard queues.

Second question: how do events go from the server to client(s)? We could again
have a queue, and the client will drain it whenever that's convenient.

Essentially what we need is the concept of pipes from unix. That would be
extremely convenient. Then the kernel just stuffs events into a pipe which
leads to the window server, and the window server puts the events into other
pipes leading to clients.

Graphical applications will have a sort of event loop where they drain the
pipes, handle all inputs, then yield and retry later.

We could even make this efficient by having a "wait on pipe" syscall which
puts the application to sleep until the pipe is nonempty.

It turns out that this is exactly the approach used in the X window system:

	https://en.wikipedia.org/wiki/Event_loop#X_Window_System

	X applications using Xlib directly are built around the XNextEvent family
	of functions; XNextEvent blocks until an event appears on the event queue,
	whereupon the application processes it appropriately.

Third question: how can a client application "talk back" to the window server,
telling it to draw its canvas? 

I guess that's a small implementation detail. We could use IPC to share the
canvas. That would probably work well. Then the client can signal the server
that it has drawn to the canvas somehow, e.g. via pipes.



For now the most important steps are (1) a mouse driver, and (2) a pipe
mechanism.

Let's start with the mouse driver. We need the OS to get events from the mouse
and handle those (by later putting them in the display server's queue).

According to the QEMU manual, it emulates a "PS/2 mouse and keyboard". That's
the purple/green 6-pin socket on some old motherboards, looking like this:
	
	https://en.wikipedia.org/wiki/File:Ps-2-ports.jpg

Eventually we should also make a USB mouse driver, but for now let's go with
this.

Relevant linkes:

	http://wiki.osdev.org/Mouse_Input
	http://wiki.osdev.org/PS/2_Mouse
	http://wiki.osdev.org/PS/2_Keyboard

I guess the current keyboard driver must also work with the PS/2 interface.
Let's look at that again..

It uses the inb/outb instructions. This sends or receives a byte at a time
from a (serial?) port.

To read a byte of input, the OS first reads a byte from the status port using
the inb instruction. This tells it whether there's new input. If so, it uses
the inb instruction with the keyboard input data port (0x60). This gives a
character at a time.

The OS doesn't continuously check for new input. It only does the above
process when kbd_intr() is called. kbd_intr() repeatedly reads a char and puts
it into a circular buffer. Then a call to getchar() drains that buffer.

How do we initialize the keyboard? Well, there's the PIC which determines
which interrupts are registered based on a bitmask. In the function kbd_init()
we change this mask so that we no longer ignore keyboard interrupts.


So to get the PS/2 mouse working, we must first enable it, and then we must
write functions which use inb to get mouse events. These events must be stored
in a buffer.

The kernel handles the KBD interrupt, draining the buffers when it happens.


A PS/2 mouse generates input on the same input port as the keyboard.

However note that IRQ 1 (IRQ_KBD) is *not* generated when mouse events occur.
Instead IRQ 12 is generated. So we'll have to additionally handle that.


Hmm.. For now we get lots of packets from the mouse, but I don't think it
makes sense to send that many events to the display server. The kernel should
process the packets. It will just keep track of the mouse x/y coordinate.
Then, once a burst of packets has been received, we can generate a single
event for "mouse moved".


Now I am getting proper packets from the mouse, but only when moving it.
Clicks are not properly registered. I wonder what's wrong..




